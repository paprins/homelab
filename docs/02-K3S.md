# Setting up k3s

[k3s](https://k3s.io/) is a lightweight Kubernetes distribution built for resource-constrained environments like Raspberry Pis. It packages the entire Kubernetes control plane into a single binary (~70 MB) and runs comfortably on ARM64 hardware.

It gives you a fully conformant Kubernetes cluster — `kubectl`, Helm, Ingress, and everything else work exactly the same as on a "full" Kubernetes setup.

## Prerequisites

- Three Raspberry Pis prepared according to [01-RASPBERRYPI.md](01-RASPBERRYPI.md)
- Static IPs configured and SSH access working
- `curl` installed on each Pi (included by default on Raspberry Pi OS)

## Architecture: server vs agent

A k3s cluster has two types of nodes:

- **Server** (also called control-plane) — Runs the Kubernetes API server, scheduler, controller manager, and etcd (the cluster database). This is the brain of the cluster. Workloads can also run here unless you add a taint.
- **Agent** (also called worker) — Runs your application workloads. Agents connect to the server and receive instructions on which Pods to run.

In this homelab I use **1 server + 2 agents**. This is not highly available (if the server dies, you cannot manage the cluster), but it is simple and works well for learning.

```
                 ┌───────────────────────┐
                 │   Server (master)     │
                 │   api.k3s.foobar.ninja│
                 │                       │
                 │   API server          │
                 │   etcd database       │
                 │   scheduler           │
                 │   + runs workloads    │
                 └───────┬───────────────┘
                         │
              ┌──────────┴──────────┐
              │                     │
    ┌─────────▼─────────┐ ┌────────▼──────────┐
    │   Agent (node1)   │ │   Agent (node2)   │
    │   node1.k3s.      │ │   node2.k3s.      │
    │   foobar.ninja    │ │   foobar.ninja    │
    │                   │ │                   │
    │   runs workloads  │ │   runs workloads  │
    └───────────────────┘ └───────────────────┘
```

## 1. Install the server node

SSH into the Pi that will be your server (e.g. `10.0.1.97`):

```bash
ssh pi@10.0.1.97
```

Run the k3s installer:

```bash
curl -sfL https://get.k3s.io | sh -s - server \
  --cluster-init \
  --disable servicelb \
  --disable traefik \
  --node-name api.k3s.foobar.ninja
```

### What these flags do

| Flag | Purpose |
|------|---------|
| `server` | Install as a server (control-plane) node |
| `--cluster-init` | Use embedded etcd instead of SQLite. This is required if you ever want to add more server nodes for high availability. Even with a single server, it is good practice. |
| `--disable servicelb` | Do not install the built-in ServiceLB (Klipper). I use [MetalLB](03-METALLB.md) instead, which is more flexible. |
| `--disable traefik` | Do not install the built-in Traefik. I install [Traefik via Helm](04-TRAEFIK.md) instead, which gives full control over the version and configuration. |
| `--node-name` | Sets a human-readable name for this node. Without it, k3s uses the machine's hostname. |

> **Why disable the built-in ServiceLB and Traefik?** k3s bundles a simple ServiceLB (Klipper) and Traefik by default. They work, but MetalLB gives you proper LoadBalancer IP management on bare metal, and installing Traefik via Helm lets you manage the version and config yourself. If two ServiceLBs or two Traefik instances run at the same time, they conflict.

The installer takes about a minute. When it finishes, k3s is running as a systemd service.

Verify the server node is ready:

```bash
sudo kubectl get nodes
```

You should see:

```
NAME                     STATUS   ROLES                       AGE   VERSION
api.k3s.foobar.ninja     Ready    control-plane,etcd,master   30s   v1.34.4+k3s1
```

## 2. Get the node token

The agent nodes need a token to join the cluster. This token is generated during server installation:

```bash
sudo cat /var/lib/rancher/k3s/server/node-token
```

Copy this token — you will need it in the next step.

> **What is the node token?** It is a shared secret that proves a node is allowed to join this cluster. Without it, any machine on your network could join. Keep it private.

## 3. Install the agent nodes

SSH into each agent node and run the installer with the token and server address.

**Agent 1** (`10.0.1.85`):

```bash
ssh pi@10.0.1.85

curl -sfL https://get.k3s.io | K3S_TOKEN=<YOUR_NODE_TOKEN> sh -s - agent \
  --server https://api.k3s.foobar.ninja:6443 \
  --node-name node1.k3s.foobar.ninja
```

**Agent 2** (`10.0.1.70`):

```bash
ssh pi@10.0.1.70

curl -sfL https://get.k3s.io | K3S_TOKEN=<YOUR_NODE_TOKEN> sh -s - agent \
  --server https://api.k3s.foobar.ninja:6443 \
  --node-name node2.k3s.foobar.ninja
```

Replace `<YOUR_NODE_TOKEN>` with the token from step 2.

| Flag | Purpose |
|------|---------|
| `K3S_TOKEN` | The token from the server node, authorizes joining the cluster |
| `agent` | Install as an agent (worker) node |
| `--server` | The URL of the server node's API. Port 6443 is the Kubernetes API port. |
| `--node-name` | Human-readable name for this agent node |

> **About the server address:** The agents connect to `https://api.k3s.foobar.ninja:6443`. For this to work, `api.k3s.foobar.ninja` must resolve to the server's IP (`10.0.1.97`). You can either add a DNS record or add an entry to `/etc/hosts` on each agent:
> ```
> 10.0.1.97  api.k3s.foobar.ninja
> ```
> Alternatively, you can use the IP directly: `--server https://10.0.1.97:6443`.

## 4. Verify the cluster

Back on the server node, check that all nodes have joined:

```bash
sudo kubectl get nodes -o wide
```

You should see all three nodes with `Ready` status:

```
NAME                     STATUS   ROLES                  INTERNAL-IP   OS-IMAGE
api.k3s.foobar.ninja     Ready    control-plane,etcd     10.0.1.97     Debian GNU/Linux 13 (trixie)
node1.k3s.foobar.ninja   Ready    <none>                 10.0.1.85     Debian GNU/Linux 13 (trixie)
node2.k3s.foobar.ninja   Ready    <none>                 10.0.1.70     Debian GNU/Linux 13 (trixie)
```

The agents show `<none>` under ROLES — this is normal. They are worker nodes.

## 5. Connect from your local machine

So far you have been running `kubectl` on the server node via SSH. It is much more convenient to manage the cluster from your own laptop.

### Copy the kubeconfig

On the server node, print the kubeconfig file:

```bash
sudo cat /etc/rancher/k3s/k3s.yaml
```

On your laptop, save this as `~/.kube/k3s-config.yml`:

```bash
# Create the .kube directory if it doesn't exist
mkdir -p ~/.kube

# Paste the contents into this file
nano ~/.kube/k3s-config.yml
```

### Update the server address

The kubeconfig file contains `server: https://127.0.0.1:6443` by default — that points to localhost, which only works on the server node itself. Change it to the server's actual IP or hostname:

```yaml
# Change this:
server: https://127.0.0.1:6443

# To this:
server: https://api.k3s.foobar.ninja:6443
```

### Set the KUBECONFIG environment variable

Tell `kubectl` to use this config file:

```bash
export KUBECONFIG=~/.kube/k3s-config.yml
```

Add this line to your shell profile (`~/.zshrc` or `~/.bashrc`) so it persists across terminal sessions:

```bash
echo 'export KUBECONFIG=~/.kube/k3s-config.yml' >> ~/.zshrc
```

### Test the connection

```bash
kubectl cluster-info
```

```
Kubernetes control plane is running at https://api.k3s.foobar.ninja:6443
CoreDNS is running at https://api.k3s.foobar.ninja:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://api.k3s.foobar.ninja:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy
```

```bash
kubectl get nodes
```

```
NAME                     STATUS   ROLES                  AGE   VERSION
api.k3s.foobar.ninja     Ready    control-plane,etcd     10m   v1.34.4+k3s1
node1.k3s.foobar.ninja   Ready    <none>                 5m    v1.34.4+k3s1
node2.k3s.foobar.ninja   Ready    <none>                 5m    v1.34.4+k3s1
```

```bash
kubectl get namespaces
```

```
NAME              STATUS   AGE
default           Active   10m
kube-node-lease   Active   10m
kube-public       Active   10m
kube-system       Active   10m
```

If you see your three nodes, you are connected. All `kubectl` and `helm` commands in the remaining guides are run from your local machine.

## Install kubectl and Helm

If you do not have `kubectl` and `helm` installed on your laptop yet:

**macOS (via Homebrew):**

```bash
brew install kubectl helm
```

**Linux:**

```bash
# kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl && sudo mv kubectl /usr/local/bin/

# Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```

## What is running after a fresh install?

A fresh k3s cluster comes with a few built-in components in the `kube-system` namespace:

```bash
kubectl get pods -n kube-system
```

| Component | What it does |
|-----------|-------------|
| **CoreDNS** | Cluster-internal DNS. Allows Pods to find Services by name (e.g. `my-service.default.svc.cluster.local`) |
| **metrics-server** | Collects CPU/memory metrics from nodes and pods. Powers `kubectl top` |
| **local-path-provisioner** | Automatically creates PersistentVolumes on the node's local disk when a PVC is created |

Since I disabled `servicelb` and `traefik`, those are not present. Their replacements are installed in the next guides.

## Troubleshooting

### Node shows "NotReady"

Check the k3s service status on that node:

```bash
sudo systemctl status k3s        # on the server
sudo systemctl status k3s-agent  # on agents
```

Check the k3s logs:

```bash
sudo journalctl -u k3s -f        # server logs
sudo journalctl -u k3s-agent -f  # agent logs
```

Common causes:
- Network connectivity between nodes (can the agent reach port 6443 on the server?)
- Wrong node token
- cgroups not enabled (see [01-RASPBERRYPI.md](01-RASPBERRYPI.md))
- Swap still enabled

### kubectl connection refused from laptop

- Check that the server address in `~/.kube/k3s-config.yml` is correct (not `127.0.0.1`)
- Check that `KUBECONFIG` is set: `echo $KUBECONFIG`
- Check that the server node is reachable: `ping api.k3s.foobar.ninja`
- Check that port 6443 is open: `nc -zv api.k3s.foobar.ninja 6443`

### Reinstalling k3s

If something goes wrong and you want to start fresh:

```bash
# On agents
sudo /usr/local/bin/k3s-agent-uninstall.sh

# On the server
sudo /usr/local/bin/k3s-uninstall.sh
```

This removes k3s completely. You can then re-run the install commands.

## Next steps

The cluster is running but has no LoadBalancer or Ingress controller. Without those, you cannot expose services outside the cluster. Next up: [MetalLB](03-METALLB.md).
